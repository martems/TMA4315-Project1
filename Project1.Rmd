---
title: "Project 1"
author: "Martinussen & Saghagen"
date: "September 6, 2017"
output: #3rd letter intentation hierarchy
#  beamer_presentation:
###    incremental: true # or >* for one at a time
#  slidy_presentation:
#    font_adjustment: +1  
   prettydoc::html_pretty:
    theme: architect
    highlight: github
#   pdf_document:
#    toc: true
#    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(car, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
```

# Part 1: Explanatory analysis of the dataset 

```{r, quietly=TRUE}
data(SLID, package = "car")
SLID <- SLID[complete.cases(SLID), ]
ds <- SLID

colnames(ds)
dim(ds)
summary(ds)

levels(ds$sex)
levels(ds$language)

g <- ggpairs(ds)
g

lm <- lm(formula = wages~education+age+sex+language, data=SLID)
summary(lm)
```
 There is a positive correlation between education/age and wages, as expected. The general level of education is lower for older age segments, indicating that higher education has become more prevalent amongst yonger people. Women's average pay is lower than men's.
 
## Assumptions of multiple linear regression (MLR) analysis
If we want to perform MLR analysis, the following must hold true:
 
1. Wages ($\bf Y$) must be a linear response to the covariates ($\bf X$), education, age, sex and language. $\bf Y = \bf X\beta + \epsilon$.
2. The error needs to have equal and uncorrelated variance for each covariate. $\mathrm{Cov}({\epsilon}) = \sigma^2 \bf{I}$.
3. Errors need to be additive (see formula related to pt. 1).
4. Errors need to be normally distributed. $\epsilon  N_n(\bf{0}, \sigma^2 \bf{I})$.

# Part 2: Linear regression with mylm package
In this project we have created our own R-package, `mylm`, to handle linear models for Gaussian data and we use this package to analyse a dataset. 
The dataset consists of 3987 observations on the following 5 variables:
* `print.mylm`, gives the call to the mylm-function and the estimates of the coefficients
* `plot.mylm`, makes a scatter plot with fitted values and residuals
* `summary.mylm`, returns a list of summary statistics of the fitted linear model like coefficients, standard errors, test of significance etc. 
* `anova,mylm`, gives the results of a sequential ANOVA

We want to study how the qualitative variable `wages` depens on one or more explanatory variables. The package constists of the following functions: 
* `wages`, composite hourly wage rate from all jobs
* `education`, number of years in schooling
* `age`, in years
* `sex`, Male or Female
* `language`, English, French or Other

Notation:
${\bf Y}: (n \times 1)$ vector of responses
${\bf X}: (n \times p)$ design matrix
${\bf \beta}: (p \times 1)$ vector of regression paramteres inlcuding the intercept ($p=k+1$)
${\bf \varepsilon}: (n \times 1)$ vector of random errors

In the following sections formulas used to make the functions in mylm-package will be presented. The functions are used on a linear regression model and the results are presented. 

## print.mylm 

This function prints out the call and the estimates of the coefficients. 

Estimates of the regression coefficients $\hat\beta$ are calculated with the LS and ML estimator $\hat\beta=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y}$.

```{r}
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
```
WHAT IS THE INTERPRETATION OF THE PARAMTERER ESTIMATES? 

## plot.mylm 

The function makes a scatter plot with fitted value on the $x$-axis and residuals on the $y$-axis. The function uses the `ggplot` functionality from the library `ggplot2`. The function is also used to plot the observed values against the residuals. 

The fitted values and residuals calculated with the formulas $\hat{\bf Y}={\bf X}\hat{\beta}$ and $\hat{\varepsilon}&={\bf Y}-\hat{\bf Y}$. 

```{r}
plot(model1)
```

## summary.mylm

SSE is the residual sum of squares calculated by $\hat{\varepsilon}^T\hat{\varepsilon}$:
```{r}
model1$SSE
```
$\text{SST} = {\bf Y}^T({\bf I}-\frac{1}{n}{\bf 1 1}^T){\bf Y}$ is the sums of squares total:
```{r}
model1$SST
```
The degrees of freedom for a linear model is $n-p$. The degrees of freedom for this model with $p=2$ is:
```{r}
model1$df
```

Estimate for $\sigma^2$ is found by the restricted maximum likelihood estimator
$\hat{\sigma}^2=frac{\text{SSE}}{n-p}$, and $\hat{\sigma}$ is the residual standard error. 
From this, the estimated covariance matrix of the parameter estimates can be calulated by $\hat{\sigma}^2({\bf X}^T{\bf X})^{-1}$. The estimate is 
```{r}
model1$comatrix
```

The standard errors of the intercept and regression coefficient are found by taking the square root of the covariance matrix, or ${\sqrt(c_{jj})\hat{\sigma}}$, where $c_{jj}$ is diagonal element $j$ of $({\bf X}^T{\bf X})^{-1})$. 
```{r}
model1$std_error_coeff
```

We perform a $z$-test to test the significance of the estimated coefficients. The $z$-values for each coefficient $\hat{\beta_j}$ is caluclated by $\frac{\hat{\beta_j}}{\sqrt(c_{jj})\hat{\sigma}}$. 



## c)



The coefficient of determination, $R^2$ is calculated from the formula $R^2 = 1-SSE/SST$ where $\text{SST} = ${\bf Y}^T({\bf I}-\frac{1}{n}{\bf 1 1}^T){\bf Y}$ is the sums of squares total. 

We per
