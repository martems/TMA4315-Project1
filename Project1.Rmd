---
title: "Project 1"
author: "Martinussen & Saghagen"
date: "September 6, 2017"
output: #3rd letter intentation hierarchy
#  beamer_presentation:
###    incremental: true # or >* for one at a time
#  slidy_presentation:
#    font_adjustment: +1  
  prettydoc::html_pretty:
    theme: architect
    highlight: github
#   pdf_document:
#    toc: true
#    toc_depth: 2
#    engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy = TRUE, message = FALSE, warning = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(car, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
```

# Part 1: Explanatory analysis of the dataset 
We will investigate the SLID dataset from the "car" library, by drawing a scatter plot of all the variables.

```{r, quietly=TRUE}
data(SLID, package = "car")
SLID <- SLID[complete.cases(SLID), ]
g <- ggpairs(SLID)
```

## Observations
* There is a positive correlation between education/age and wages, as expected.
* The general level of education is lower for older age segments, indicating that higher education has become more prevalent amongst younger people.
* We also observe that women's average pay is lower than men's.
 
## Assumptions of multiple linear regression (MLR) analysis
If we want to perform MLR analysis, the following must hold true:
 
   1. Wages must be a linear response to the covariates, education, age, sex and language, i.e. $\bf Y = \bf X\bf \beta + \bf \varepsilon$.
   2. The error needs to have equal and uncorrelated variance for each covariate, i.e. $\text{Var}(\varepsilon) = \sigma^2 I$.
   3. Errors need to be additive (see formula related to pt. 1).
   4. Errors need to be normally distributed, i.e. $\varepsilon \sim N_n(0, \sigma^2 I)$.

# Part 2: Linear regression with mylm package
In this project we have created our own R-package, named `mylm`, to handle linear models for Gaussian data. We will use this package in order to analyze an example dataset.
The example dataset consists of $n = 3987$ observations of the following 5 variables:

* `wages`: composite hourly wage rate from all jobs.
* `education`: number of years in schooling.
* `age`: in years.
* `sex`: Male or Female.
* `language`: English, French or Other.

We want to study how the qualitative variable `wages` depends on one or more explanatory variables. The `mylm` package consists of the following functions: 

* `print.mylm`: invokes the `mylm` function and prints the result, including the estimated regression coefficients.
* `plot.mylm`: draws a scatter plot with fitted values and residuals.
* `summary.mylm`: returns a list of summary statistics of the fitted linear model, e.g. coefficients, standard errors, tests of significance, and so on.
* `anova.mylm`: gives the results of a sequential ANOVA.


### Notation:

* ${\bf Y}: (n \times 1)$ vector of responses.
* ${\bf X}: (n \times p)$ design matrix.
* ${\bf \beta}: (p \times 1)$ vector of regression parameters including the intercept ($p = k + 1$).
* ${\bf \varepsilon}: (n \times 1)$ vector of random errors.

We will now create a linear regression model using the `mylm` package, and present the results. We will also present the linear regression formulas used in order to make the functions in the `mylm` package.

## a)

This function invokes the `$call` method of the mylm object, and prints the result in addition to the resulting regression coefficients.

### Estimates of the regression coefficients
Estimates of the regression coefficients $\hat \beta$ are calculated with the least squares (LS) and most likelihood (ML) estimator 
$$
\hat \beta = ({\bf X}^T {\bf X})^{-1} {\bf X}^T {\bf Y}.
$$

An example of how to use `mylm` in order to calculate estimated correlation coefficients follows
```{r}
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
```

## b)

The estimates of the intercept and regression coefficients is presented in Part 2 a)

### $\sigma^2$, covariance matrix and standard errors of the parameter estimates
In order to find the covariance matrix and standard errors we need the estimate for $\sigma^2$. It is found by the restricted maximum likelihood estimator
$$
\hat{\sigma}^2 = \frac{\text{SSE}}{n - p},
$$
where $\hat{\sigma}$ is the residual standard error. The residual sum of squares, SSE, is presented in Part 2 d). 

From this, the estimated covariance matrix of the coefficients can be calculated by $\text{Cov}(\hat\beta)=\hat{\sigma}^2 ({\bf X}^T {\bf X})^{-1}$. The estimate of the covariance matrix for `model1` is
```{r}
model1$covmatrix
```

The standard errors of the intercept and regression coefficients are found by taking the square root of the covariance matrix, or ${\sqrt{c_{jj}}\hat{\sigma}}$, where $c_{jj}$ is diagonal element $j$ of $({\bf X}^T{\bf X})^{-1}$. For `model1` this gives the following
```{r}
model1$std_error_coeff
```

### Significance of the regression coefficients

We perform a $z$-test to test the significance of the estimated coefficients. The $z$-values for each coefficient $\hat{\beta_j}$ is calculated by 
$$
z_{\text{value}} = \frac{\hat{\beta_j}}{\sqrt{c_{jj}} \hat{\sigma}}
$$

The result from the z-test on `model1` is
```{r}
model1$z_stat
model1$pvalue_z
```

### Interpretation of the parameter estimates

The parameter estimates represent the mean estimated change in the response caused by one unit of change in the respective predictor variable while holding other predictors in the model constant. The estimated coefficient for `education` is `r model1$coefficients[2]`. The coefficient indicates that for every additional unit in `education`, we can expect `wages` to increase by an average of `r model1$coefficients[2]`.


## c) 

This function draws a scatter plot with _fitted_ values on the $x$-axis, and residuals on the $y$-axis, by utilizing the `ggplot` functionality from the `ggplot2` library. It is also used to plot the _observed_ values against the residuals. 

### Fitted values and residuals

The fitted values and residuals are calculated by the following formulas
$$\hat{\bf Y} = {\bf X}  \hat{\beta} $$
$$\hat{\varepsilon}={\bf Y} - \hat{\bf Y}$$

and can be drawn in a scatter plot as explained above in the following way
```{r, results='hide'}
plot(model1)
```


## d)

### SSE, SST and degrees of freedom
The expressions for the residual sum of squares ($\text{SSE}$) and the sums of squares total ($\text{SST}$) are
$$\text{SSE} = \hat{\varepsilon}^T \hat{\varepsilon}$$
$$\text{SST} = {\bf Y}^T ({\bf I} - \frac{1}{n} {\bf 1 1}^T){\bf Y}$$

The calculated values for $\text{SSE}$ and $\text{SST}$ for `model1` is
```{r}
model1$SSE
model1$SST
```

The degrees of freedom for a linear model is $n - p$. For this particular model we therefore have `r model1$n` $-$ `r model1$p` $=$ `r model1$df` degrees of freedom.

### Significance of the regression 

We test the significance of the regression using a $\chi^2$-test with $k = p - 1$ degrees of freedom. We test if at least one of the regression parameters is different from 0. The expression for the $\chi^2$-statistics is 
$$
\chi_k^2 = \frac{\text{SST} - \text{SSE}} {\frac{\text{SSE}}{n - p}}.
$$
The critical values for the tests are
```{r}
model1$critical_z
model1$critical_chi
```

The $z$-test is a single hypothesis test where we are interested in testing one null hypothesis against an alternative hypothesis. We test if a regression parameter $\beta_j$ is significant: $H_0: \beta_j = 0 \text{ vs. } H_1: \beta_j \neq 0$. Here we assume that all other regression parameters are _not_ equal to zero.

In the $\chi^2$-test we are testing "significance of regression": $H_0: \beta_1 = \beta_2 = \cdots \beta_k = 0 \text{ vs. }.H_1 \text{: at least one different from zero}.

This means we test if at least one of the regression coefficients is different from 0.

### ANOVA 

In the ANOVA function, the $\chi^2$-test is calculated as when the testing linear hypotheses, but with some alterations. Instead of comparing the full regression model to a smaller model, we are now looking at the change in SSE between two smaller models. 

Let $A$ be the full regression model and $B1$ and $B2$ be two smaller models, for example $B1$ with only intercept and $B2$ with intercept _and_ education. Then the test statistics is:
$$ \chi_\text{dfB1-dfB2}^2=\frac{\text{SSE}_{B1}-\text{SSE}_{B2}}{\frac{\text{SSE}_A}{\text{df}_A}}$$
The complete output of the function `anova` with `model1`
```{r}
anova(model1)
```

## e) 

The coefficient of determination, $R^2$ is estimated with the formula $R^2 = 1-SSE/SST$. $R^2$ for this model is `r model1$R_squared`. $R^2$ can take a value between 0 and 1 and closer it is to 1, the better the fit to the data. 

## f)

The Pearson's linear correlation coefficient is a measure of the linear correlation between two variables $x_i$ and $x_j$. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. The expression is 
$$r = \frac{\text{cov}(x_i,x_j)}{\sigma_{x_i}\sigma_{x_j}}$$ 
The numerical values for `model1` is

```{r}
model1$linear_corr_coeff
```

In `model1`, there is a sligth negative correlation. Hence, an increase in `education` is associated with a decrease in `age`.

### Output of `summary`

The complete output of the function `summary` with `model1`
```{r}
summary(model1)
```

# Part 3: Multiple linear regression 

## a)

We fit a linear regression model to the data with `wages` as the response with `education` and `age` as predictors.

```{r}
model2 <- mylm(wages ~ education+age, data = SLID)
```

## b)

```{r}
summary(model2)
```

The estimates and standard errors of the intercepts and regression coefficients for `model2` is presented in the summary above. From the $z$-test we can see that all the regression paramteres is significant in the model.  

## c)

We fit two simple linear regression with only age and only education to compare with `model2`. The estimates of the coefficients for the two models are presented below. 

```{r}
model2onlyage <- mylm(wages ~ age, data = SLID)
model2onlyage$coefficients
model1$coefficients
```

The parameters do differ when the elements of $\hat \beta$ are correlated. We know that $\text{Cov}(\hat \beta) = \hat\sigma^2(\bf X^T \bf X)^{-1}$. From this we can conclude that the parameter estimate differ when the columns of the design matrix $X$ are orthogonal. The parameter estimates in these models differ because the design matrix column for `age` is not orthogonal to the design matrix column for `education`.


# Part 4: Analysis of variance (ANOVA)

## a)

We fit a linear regression model to the data with `wages` as the response with `sex` and `language` as covariates. `model3a` is using dummy variable coding and `model3b` is using effect coding.
```{r}
model3a <- mylm(wages ~ sex + language, data = SLID)
summary(model3a)
```

Here we have performed a dummy variable coding of the categorical covariates, where `female` has become the reference category for `sex` and `english` has become the reference category for `language`. This is due to the alphabetical ordering of the categories. 

This means that being an english speaking woman is counted as being part of the average of the intercept, loosely speaking. As an example, a male is expected to have `r model3a$coefficients[2]` more units of wage compared to a female. 

```{r}
model3b <- mylm(wages ~ sex + language, data = SLID, contrasts = list(language = "contr.sum",
sex = "contr.sum"))
summary(model3b)
```

Here we have performed an effect coding of the categorical covariates. This mean that the sum of the effects for the levels of the factor sums to zero. You can find the "missing" coefficient of the reference category by negating the sum of the other coefficients (related to the same covariate). Compared to the dummy variable coding, there is no real case where only the intercept is used. 

In this coding scheme, `male` and `otherLanguage` are the "missing" categories. As an example, the missing coefficient for `otherLanguage` can be calculated by -(`r model3b$coefficients[3]` + `r model3b$coefficients[4]`) = `r -(model3b$coefficients[3] + model3b$coefficients[4])`.

TODO: utdrag fra modelmatrix X

## b) 

```{r}
anova(model3a)
anova(model3b)
```

Sequential anova of the two models `model3a` and `model3b` give the same result. `Sex` is significant when it is the only covariate in the model. `Language` is not significant when added to the model with `intercept` and `sex`.

```{r, eval=FALSE}
anova(mylm(wages ~ sex + language, data = SLID))
and
anova(mylm(wages ~ language + sex, data = SLID))
```

The commands above give different results because the terms are added sequantially. The commands produce a sequential table of the reductions in residual sum of squares (SSE) as each term in the regression formula is added in turn. Therefore the reductions in SSE will differ as the terms are added in different orders.  

## c)

```{r}
model4a <- mylm(wages ~ sex + age + sex*age, data = SLID)
model4b <- mylm(wages ~ sex + age + sex*age, data = SLID, contrasts = list(language = "contr.sum",
sex = "contr.sum"))
```

In `model4a` and `model4b` we have added an interaction term to the model where the two predictor variables are multiplied. The presence of a non-zero interaction indicates that the effect of one predictor variable on the response is different at different values of the other predictor variable.

```{r}
model4a$coefficients
```

If the interaction effect is non-zero, the interpretation of the coefficients changes. Let us explain this table further.

* In the case of `sexFemale`, we expect a base rate of `r model4a$coefficients[1]` units of wage plus `r model4a$coefficients[3]` for each additional year of age. 

* For the case of `sexMale` we have a reduction of the base rate equal to `r model4a$coefficients[2]`, but this is _more_ than compensated for by the additional increase in pay for each year of age, `r model4a$coefficients[4]`.


```{r}
model4b$coefficients
```

The interpretation for this table goes along the same lines as for the interpretation for the coefficient table from task 4b) related to effect coding.

* A male is expected to have a base rate of wage equal to `r model4b$coefficients[1]` + (-1) $*$ `r model4b$coefficients[2]` plus a wage increase for each additional year of age equal to `r model4b$coefficients[3]` + (-1) $*$ `r model4b$coefficients[4]`. 

* A female is expected to have a base rate of wage equal to `r model4b$coefficients[1]` + 1 $*$ `r model4b$coefficients[2]` plus a wage increase for each additional year of age equal to `r model4b$coefficients[3]` + 1 $*$ `r model4b$coefficients[4]`. 

In other words, men are expected to have a greater benefit of getting older when it comes to wage, compared to women. 




TODO:

Test whether regression coefficients have a common slope or not. Plot the regression lines for the different
values of sex in the same plot, and perform the test using the plot (use ggplot). Describe the graph. What
is the conclusion? How can the test be performed without graphs? Does that give the same conclusion?

# Part 5: Testing the `mylm` package

```{r}
education2 = SLID$education^2 #square of education
model5a <- mylm(wages ~ sex + age + language + education2 , data = SLID)
model5b <- mylm(wages ~ language + age + language*age, data = SLID)
model5c <- mylm(wages ~ education-1, data = SLID)
```

```{r}
summary(model5a)
```

From the summary we see that `sexMale`, `age` and `education2` are significant in the model. Increase in `age` and `education` results in a higher expected level of `wage`. The same goes for being male instead of female. 

$R^2=$ `r model5a$R_squared`, so the model only explains 30% of the variability in the data. A possible improvement to the model would be to exclude `language` from the fitted variables. 


```{r}
summary(model5b)
```

This model seems to have less explaining power than the previous one, as $R^2=$ `r model5b$R_squared`. The only covariate significant at a 0.001 level is `age`. The other semi-significant effect is the interaction between `languageFrench` and `age`. We can therefore say with some amount of certainty that french speaking people are expected to less degree of wage increase related to increased age.

We are satisifed with fitting `language` as part of the model, especially when we compensate for the interaction effects. A possible improvement to the model is to add `education` as one of the fitted covariates. 

```{r}
anova(model5b)
```

The analysis of variance confirms that the interaction effect is the most cruical part of fitting `language` as a covariate. Still, the interaction effect has a relatively small explaining power relative to `age`.


```{r}
summary(model5c)
```

We can say with great certainty that `education` has a positive effect on `wage`. This is based on the value of the z-test, `r model5c$z_stat[1,1]`, resulting in p-value indistinguishable from 0.  

There is no reason to believe that people without any education have zero wages. The result is that the coefficient for `education` becomes greater than it really is, in order to compensate for the missing intercept. A possible improvement is to include an intercept in the model. 



