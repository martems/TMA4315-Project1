---
title: "Project 1"
author: "Martinussen & Saghagen"
date: "September 6, 2017"
output: #3rd letter intentation hierarchy
#  beamer_presentation:
###    incremental: true # or >* for one at a time
#  slidy_presentation:
#    font_adjustment: +1  
  prettydoc::html_pretty:
    theme: architect
    highlight: github
#   pdf_document:
#    toc: true
#    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy = TRUE, message = FALSE, warning = FALSE)
library(ggplot2, quietly = TRUE, warn.conflicts = FALSE)
library(car, quietly = TRUE, warn.conflicts = FALSE)
library(GGally, quietly = TRUE, warn.conflicts = FALSE)
```

# Part 1: Explanatory analysis of the dataset 
We will investigate the SLID dataset from the "car" library, by drawing a scatter plot of all the variables.

```{r, quietly=TRUE}
data(SLID, package = "car")
SLID <- SLID[complete.cases(SLID), ]
g <- ggpairs(SLID)
```

## Observations
* There is a positive correlation between education/age and wages, as expected.
* The general level of education is lower for older age segments, indicating that higher education has become more prevalent amongst younger people.
* We also observe that women's average pay is lower than men's.
 
## Assumptions of multiple linear regression (MLR) analysis
If we want to perform MLR analysis, the following must hold true:
 
   1. Wages must be a linear response to the covariates, education, age, sex and language, i.e. $\bf Y = \bf X\bf \beta + \bf \epsilon$.
   2. The error needs to have equal and uncorrelated variance for each covariate, i.e. $\text{Var}(\varepsilon) = \sigma^2 I$.
   3. Errors need to be additive (see formula related to pt. 1).
   4. Errors need to be normally distributed, i.e. $\varepsilon \sim N_n(0, \sigma^2 I)$.

# Part 2: Linear regression with mylm package
In this project we have created our own R-package, named `mylm`, to handle linear models for Gaussian data. We will use this package in order to analyze an example dataset.
The example dataset consists of $n = 3987$ observations of the following 5 variables:

* `wages`: composite hourly wage rate from all jobs.
* `education`: number of years in schooling.
* `age`: in years.
* `sex`: Male or Female.
* `language`: English, French or Other.

We want to study how the qualitative variable `wages` depends on one or more explanatory variables. The `mylm` package consists of the following functions: 

* `print.mylm`: invokes the `mylm` function and prints the result, including the estimated regression coefficients.
* `plot.mylm`: draws a scatter plot with fitted values and residuals.
* `summary.mylm`: returns a list of summary statistics of the fitted linear model, e.g. coefficients, standard errors, tests of significance, and so on.
* `anova.mylm`: gives the results of a sequential ANOVA.


### Notation:

* ${\bf Y}: (n \times 1)$ vector of responses.
* ${\bf X}: (n \times p)$ design matrix.
* ${\bf \beta}: (p \times 1)$ vector of regression parameters including the intercept ($p = k + 1$).
* ${\bf \varepsilon}: (n \times 1)$ vector of random errors.

We will now create a linear regression model using the `mylm` package, and present the results. We will also present the linear regression formulas used in order to make the functions in the `mylm` package.

## `print.mylm`

This function invokes the `$call` method of the mylm object, and prints the result in addition to the resulting regression coefficients.

### Estimates of the regression coefficients
Estimates of the regression coefficients $\hat \beta$ are calculated with the least squares (LS) and most likelihood (ML) estimator 
$$
\hat \beta = ({\bf X}^T {\bf X})^{-1} {\bf X}^T {\bf Y}.
$$

An example of how to use `mylm` in order to calculate estimated correlation coefficients follows
```{r}
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
```
The parameter estimates represent the mean estimated change in the response caused by one unit of change in the respective predictor variable while holding other predictors in the model constant. The estimated coefficient for `education` is `r model1$coefficients[2]`. The coefficient indicates that for every additional unit in `education`, we can expect `wages` to increase by an average of `r model1$coefficients[2]`.

## `plot.mylm` 

This function draws a scatter plot with _fitted_ values on the $x$-axis, and residuals on the $y$-axis, by utilizing the `ggplot` functionality from the `ggplot2` library. It is also used to plot the _observed_ values against the residuals. 

### Fitted values and residuals

The fitted values and residuals are calculated by the following formulas
$$
\hat{\bf Y} = {\bf X} \hat{\beta} 
\hat{\varepsilon}={\bf Y} - \hat{\bf Y}.
$$

And can be drawn in a scatter plot as explained above in the following way
```{r, results='hide'}
plot(model1)
```

## `summary.mylm`
`summary.mylm` prints out properties of the linear model intended for analytical purposes, such as...

### SSE and SST
The expressions for the residual sum of squares ($\text{SSE}$) and the sums of squares total ($\text{SST}$) are
$$
\text{SSE} = \hat{\varepsilon}^T \hat{\varepsilon}
\text{SST} = {\bf Y}^T ({\bf I} - \frac{1}{n} {\bf 1 1}^T){\bf Y}
$$

The calculated values for $\text{SSE}$ and $\text{SST}$ for `model1` is
```{r}
model1$SSE
model1$SST
```

The degrees of freedom for a linear model is $n - p$. For this particular model we therefore have `r model1$df` degrees of freedom.

### $\hat{\sigma}^2$, covariance matrix and standard erros

Estimate for $\sigma^2$ is found by the restricted maximum likelihood estimator
$$\hat{\sigma}^2=\frac{\text{SSE}}{n-p}$$
 and $\hat{\sigma}$ is the residual standard error. 
From this, the estimated covariance matrix of the parameter estimates can be calulated by $\hat{\sigma}^2({\bf X}^T{\bf X})^{-1}$. The estimate is 
```{r}
model1$covmatrix
```

The standard errors of the intercept and regression coefficient are found by taking the square root of the covariance matrix, or ${\sqrt{c_{jj}}\hat{\sigma}}$, where $c_{jj}$ is diagonal element $j$ of $({\bf X}^T{\bf X})^{-1}$. 
```{r}
model1$std_error_coeff
```

### Significance of estimated coefficients and significance of the regression 

We perform a $z$-test to test the significance of the estimated coefficients. The $z$-values for each coefficient $\hat{\beta_j}$ is caluclated by 
$$z_{value}=\frac{\hat{\beta_j}}{\sqrt{c_{jj}}\hat{\sigma}}$$

We test the significance of the regression using a $\chi^2$-test with $k=p-1$ degrees of freedom. We test if at least one of the regression pramteres is different from 0. The expression for the $\chi^2$-statistics is 
$$\chi_k^2=\frac{\text{SST}-\text{SSE}}{\frac{\text{SSE}}{n-p}}$$
The critical values for the tests are
```{r}
model1$critical_z
model1$critical_chi
```

The $z$-test is a single hypothesis testing where we are interested in testing one null hypothesis against an alternative hypothesis. We test if a regression parameter $\beta_j$ is siginifcant: $H_0:\beta_j=0 \text{ vs. } H_1: \beta_j \neq 0$. We say that our test is done given that the other variables are present in the model are not zero. 

In the $\chi^2$-test we are testing "significance of regression": $H_0: \beta_2=\beta_2=⋯=\beta_k=0 \text{ vs. }.H_1$: at least one different from zero.This means we test if at least one of the regression parameters (in addition to the intercept) is different from 0.

### $R^2$ and Pearsons linear correlation coefficient

The coefficient of determination, $R^2$ is estimated with the formula $R^2 = 1-SSE/SST$. $R^2$ for this model is `r model1$R_squared`. $R^2$ can take a value between 0 and 1 and closer it is to 1, the better the fit to the data. 

The Pearson's linear correlation coefficient is a measure of the linear correlation between two variables $x_i$ and $x_j$. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. The expression is 
$$r = \frac{\text{cov}(x_i,x_j)}{\sigma_{x_i}\sigma_{x_j}}$$ 
The numerical values for `model1` is

```{r}
model1$linear_corr_coeff
```

In `model1`, there is a sligth negative correlation. Hence, an increase in `education` is associated with a decrease in `age`.

The complete output of the function `summary` with `model1`
```{r}
summary(model1)
```

# anova.mylm

In the anova function, the $\chi^2$-test is calculated as for when testing linear hypotheses, but with som changes. Instead of comparing the full regression model to a small model, we are now looking at the change in SSE between two smaller models. 

Let $A$ be the full regression model and $B1$ and $B2$ be two smaller models, for example $B1$ with only intercept and $B2$ with intercept and education. Then the test statistics is:
$$ \chi_\text{dfB1-dfB2}^2=\frac{\text{SSE}_{B1}-\text{SSE}_{B2}}{\frac{\text{SSE}_A}{\text{df}_A}}$$
The complete output of the function `anova` with `model1`
```{r}
anova(model1)
```

# Part 3: Multiple linear regression 

We fit a linear regression model to the data with `wages` as the response with `education` and `age` as predictors.

```{r}
model2 <- mylm(wages ~ education+age, data = SLID)
summary(model2)
```

The estimates and standard errors of the intercepts and regression coefficients for `model2` is presented in the summary above. From the "z"-test we can see that both `education` and `age` is significant in the model. The regression coefficients show that `education` has more effect on the response than `age`. 


We fit two simple linear regression with only age and only education to compare with `model2`. The estimates of the coefficients for the two models are presented below. 

```{r}
model2onlyage <- mylm(wages ~ age, data = SLID)
model2onlyage$coefficients
model1$coefficients
```

The parameter estimates differ because the model matrix $X$ changes as we add or remove terms. When does the parameter estimates differ?
Explain/show how you can use mylm to find these values.

# Part 4: Analysis of variance (ANOVA)

We fit a linear regression model to the data with `wages` as the response with `sex` and `language` as covariates. `model3a` is using dummy variable coding and `model3b` is using effect coding.
```{r}
model3a <- mylm(wages ~ sex + language, data = SLID)
model3b <- mylm(wages ~ sex + language, data = SLID, contrasts = list(language = "contr.sum",
sex = "contr.sum"))
```

```{r}
summary(model3a)
summary(model3b)
```

Estimates and standard error of the intercept and effects of factors are found in the summarys. In both models, `intercept` and `sex` are significant from the $z$-test. The parameter estimates is different in the two models. In the `model3b` with effect coding we can see that the estimates of the coefficients are smaller than with dummy variable coding, while the intercept is bigger. 

```{r}
model3a$Y_hat
summary
```


SKRIVE GJERNE NOE MER 

```{r}
anova(model3a)
anova(model3b)
```

Sequential anova of the two models `model3a` and `model3b` give the same result. `Sex` is significant when only covariate in the model. `Language` is not significant when added to the model with `intercept` and `sex`.


```{r, eval=FALSE}
anova(mylm(wages ~ sex + language, data = SLID))
and
anova(mylm(wages ~ language + sex, data = SLID))
```

The commands above give different results because the terms are added sequantially. The commands produce a sequential table of the reductions in residual sum of squares (SSE) as each term in the regression formula is added in turn. Therefore the reductions in SSE will differ as the terms are added in different orders.  

```{r}
model4a <- mylm(wages ~ sex + age + sex*age, data = SLID)
model4b <- mylm(wages ~ sex + age + sex*age, data = SLID, contrasts = list(language = "contr.sum",
sex = "contr.sum"))
```

In `model4a` and `model4b` have added an interaction term to the model where the two predictor variablaes are multiplied. The presence of a significant interaction indicates that the effect of one predictor variable on the response variable is different at different values of the other predictor variable. 

```{r}
model4a$coefficients
model4b$coefficients
```

Adding an interaction term to the model changes the interpretation of all of the coefficients. If there were no interaction term, the coefficient for `age` would be interpreted as the unique effect of `age` one the respons. But the interaction means that the effect of `age` is different for different values of `sex`.  So the unique effect of `sage` on `wages` also depends on the values of the interaction coefficient and `sex`. The unique effect of `age` is represented by everything that is multiplied by `age` in the model. The coefficient for `age` alone is now interpreted as the unique effect of `age` on only when `sex`= 0.

MÅ GJØRES-------

Test whether regression coefficients have a common slope or not. Plot the regression lines for the different
values of sex in the same plot, and perform the test using the plot (use ggplot). Describe the graph. What
is the conclusion? How can the test be performed without graphs? Does that give the same conclusion?

# Part 5: Testing the `mylm` package

```{r}
education2 = SLID$education^2 #square eudcation
model5a <- mylm(wages ~ sex + age + language + education2 , data = SLID)
model5b <- mylm(wages ~ language + age + language*age, data = SLID)
model5c <- mylm(wages ~ education-1, data = SLID)
```

```{r}
summary(model5a)
anova(model5a)
```

From the summary we see that `sexMale`, `age` and `education2` are significant in the model. We see that all the singificant covariates have a positive impact on the response. $R^2=$ `r model5a$R_squared`, so the model only explain 30% of the variablility in the data. Language is not significiant so an idea would be to remove this from the model. 

MÅ KOMMENTERE NOE PÅ ANOVA 

```{r}
summary(model5b)
anova(model5b)
```
The significant terms are `intercept`, `age` and `languageFrench:age`. 

mention one small change that could make the model better.

```{r}
summary(model5c)
anova(model5c)
```
MÅ GJØRES----
Based on the results from each model, write a few sentences about the interpretation and significance of the
parameters, and mention one small change that could make the model better.

